import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.utils.model_zoo as model_zoo
import torch.nn.functional as F

import sys
sys.path.append('../')
__all__=['model']



model_urls = {
    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',
    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',
    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',
    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',
    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',
    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',
    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',
    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',
}

class VGG(nn.Module):
    def __init__(self,features,num_classes=9,args=None,threshold=None):
        super(VGG,self).__init__()
        self.features=features
        self.cls=self.classifier(512,num_classes)
        self._initialize_weights()

        self.loss_cross_entropy=nn.CrossEntropyLoss()

    def classifier(self,in_planes,out_planes):
        return nn.Sequential(
            nn.Conv2d(in_planes,1024,kernel_size=3,padding=1,dilation=1),
            nn.ReLU(True),
            nn.Conv2d(1024,1024,kernel_size=3,padding=1,dilation=1),
            nn.ReLU(True),
            nn.Conv2d(1024,out_planes,kernel_size=1,padding=0) )
    def forward(self, x,label=None):
        x=self.features(x)
        x=F.avg_pool2d(x,kernel_size=3,stride=1,padding=1)
        out=self.cls(x)
        self.map1=out
        logits_1=torch.mean(torch.mean(out,dim=2),dim=2)
        return [logits_1,]
    def get_loss(self,logits,gt_labels):
        if self.onehot=='True':
            gt=gt_labels.float()
        else:
            gt=gt_labels.long()
        loss_cls=self.loss_cross_entropy(logits[0],gt)
        return [loss_cls,]
    def get_localization_maps(self):
        return self.normalize_atten_maps(self.map1)
    def get_heatmaps(self,gt_label):
        map1=self.get_atten_map(self.map1,gt_label)
        return [map1,]
    def get_fused_heatmap(self,gt_label):
        maps=self.get_heatmaps(gt_label=gt_label)
        fuse_atten=maps[0]
        return fuse_atten
    def get_maps(self,gt_label):
        map1=self.get_atten_map(self.map1,gt_label)
        return [map1,]
    def normalize_atten_maps(self,atten_maps):
        atten_shape=atten_maps.size()

        batch_mins,_=torch.min(atten_maps.view(atten_shape[0:-2]+(-1,)),dim=-1,keepdim=True)
        batch_maxs,_=torch.max(atten_maps.view(atten_shape[0:-2]+(-1,)),dim=-1,keepdim=True)
        atten_normed=torch.div(atten_maps.view(atten_shape[0:-2]+(-1,))-batch_mins,
                               batch_maxs-batch_mins)
        atten_normed=atten_normed.view(atten_shape)
        return atten_normed
    def get_atten_map(self,feature_maps,gt_labels,normalize=True):
        label=gt_labels.long()
        feature_map_size=feature_maps.size()
        batch_size=feature_map_size[0]
        atten_map=torch.zeros([feature_map_size[0],feature_map_size[2],feature_map_size[3]])
        atten_map=Variable(atten_map.cuda())
        for batch_idx in range(batch_size):
            atten_map[batch_idx,:,:]=torch.squeeze(feature_maps[batch_idx,label.data[batch_idx],:,:])
        if normalize:
            atten_map=self.normalize_atten_maps(atten_map)
        return atten_map

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m,nn.Conv2d):
                nn.init.xavier_uniform_(m.weight.data)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m,nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.zero_()
            elif isinstance(m,nn.Linear):
                m.weight.data.normal_(0,0.01)
                m.bias.data.zero_()

def make_layers(cfg,dilation=None,batch_norm=False):
    layers=[]
    in_channels=3
    for v,d in zip(cfg,dilation):
        if v=='M':
            layers+=[nn.MaxPool2d(kernel_size=3,stride=2,padding=1)]
        elif v=='N':
            layers+=[nn.MaxPool2d(kernel_size=3,stride=1,padding=1)]
        else:
            conv2d=nn.Conv2d(in_channels,v,kernel_size=3,padding=d,dilation=d)
            if batch_norm:
                layers+=[conv2d,nn.BatchNorm2d(v),nn.ReLU(inplace=True)]
            else:
                layers+=[conv2d,nn.ReLU(inplace=True)]
            in_channels=v
    return nn.Sequential(*layers)

cfg = {
    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'D1': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'N', 512, 512, 512, 'N'],
    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}
dilation = {
    'D1': [1, 1, 'M', 1, 1, 'M', 1, 1, 1, 'M', 1, 1, 1, 'N', 1, 1, 1, 'N']
}

def model(pretrained=False,**kwargs):
    """ VGG 16-layer model """
    model=VGG(make_layers(cfg['D1'],dilation=dilation['D1']),**kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg16']),strict=False)
    return model